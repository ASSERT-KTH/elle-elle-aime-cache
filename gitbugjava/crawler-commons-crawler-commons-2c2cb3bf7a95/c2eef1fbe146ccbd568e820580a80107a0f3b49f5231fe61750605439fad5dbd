{
    "generation": "private SimpleRobotRules parseContent(String url, byte[] content, String contentType, Collection<String> robotNames, boolean exactUserAgentMatching) {\n\n    //... (rest of the method remains the same until the lineParser)\n\n    while (lineParser.hasMoreTokens()) {\n        String line = lineParser.nextToken();\n\n        // Get rid of HTML markup, in case some brain-dead webmaster has\n        // created an HTML\n        // page for robots.txt. We could do more sophisticated processing\n        // here to better\n        // handle bad HTML, but that's a very tiny percentage of all\n        // robots.txt files.\n        if (hasHTML) {\n            line = line.replaceAll(\"<[^>]+>\", \"\");\n        }\n\n        // trim out comments and whitespace\n        int hashPos = line.indexOf(\"#\");\n        if (hashPos >= 0) {\n            line = line.substring(0, hashPos);\n        }\n\n        line = line.trim();\n        if (line.length() == 0) {\n            continue;\n        }\n\n        RobotToken token = tokenize(line);\n\n        // Unescape path values (issue #29, test testUnicodeUnescapedPaths)\n        if (token.getValue()!= null) {\n            try {\n                token.setValue(java.net.URLDecoder.decode(token.getValue(), \"UTF-8\"));\n            } catch (UnsupportedEncodingException e) {\n                reportWarning(parseState, \"Unsupported encoding on path: {}\", token.getValue());\n                continue;\n            }\n        }\n\n        switch (token.getDirective()) {\n            //... (rest of the method remains the same)",
    "exact_match": false,
    "ast_match": false,
    "compile": null,
    "test": false
}